
-------------------------
Storage Account Creation
-------------------------

i) Create an storage account by giving any name 'dataforbigdata3457' along with a resource group 'datademoGroup1'.
ii) In the Basics section change the Redundancy to LRS.
iii) Leave rest all settings default by clicking on next and at last click on review + create.

After the creation of storage account click on it(dataforbigdata3457)>containers
	-> Create a container by any name(data).
	-> Upload the file of which data to be copied to sql Database


----------------------
SQL Database Creation
----------------------

i) Search for SQL databases and click on it and then click on create.
ii) Select the existing Resource Group, Enter the database name 'appdb', Create new Server name 'appserver598' and set the admin login and password to login the database.
iii) Now under Compute + storage > click on configure database and select Service tier as "Basic" and apply.
iv) Under Backup redundancy > Locally-redundant backup storage.
v) Click on Next until you reach Review + create leave all the options as default.


----------------------
Create a data factory
----------------------

i) Go to the Azure portal and search for Data Factories and select it.
ii) On the Create Data Factory page, under Basics tab, select your Azure Subscription in which you want to create the data factory.
iii) For Resource Group, Select an existing resource group from the drop-down list.
iv) For Name, enter 'logcsvunformatted'. The name of the Azure data factory must be globally unique. If you see any error in name, change the name of the data factory 
v) Select Next: Git configuration, and then select Configure Git later check box.
vi) Select Review + create, and select Create after the validation is passed. After the creation is complete, select Go to resource to navigate to the Data Factory page.
vii) Select 'Open' on the 'Open Azure Data Factory Studio' to start the Azure Data Factory user interface (UI) application on a separate browser tab.


-----------------------------------------------------------------------
Accessing the SQL database through SSMS (SQL Server Management Studio)
-----------------------------------------------------------------------

i) Open the SSMS ans click on connect.
ii) Enter the following details
	Server type : Database Engine
	Server name : copy it from appdb overview 'appserver598.database.windows.net'
	Authentication : SQL server Authentication
	Login : sqluser		(The credentials you entered while creting the server)
	Password : azure@123
iii) Click on connect and it will ask to sign in to your azure account after that it will get connected.
iv) Select the database you created earlier and use it for the query execution.
v) Create a table with the suitable columns required for the data to be transferred.
	 
	CREATE TABLE [autodata]
      (
    	  [MAKE] [varchar](200) ,
	  [FUELTYPE] [varchar](200) ,
	  [ASPIRE] [varchar](200) ,
	  [DOORS] [varchar](200) ,
	  [BODY] [varchar](100) ,
	  [DRIVE] [varchar](200) ,
	  [CYLINDERS] [varchar](200) ,
	  [HP] [int],
	  [RPM] [INT],
	  [MPG-CITY] [int],
	  [MPG-HWY] [int],
	  [PRICE] [int]
	
      )
vi) When you run the following query you see that a table has been created with the above mentioned columns with no data in it.
	select * from [autodata];


--------------------------------------------------------------------------------
Creating a pipeline that copies data from Azure Blob Storage to a SQL database:-
--------------------------------------------------------------------------------

i) In the Azure Data Factory Studio, we will use Author method to create the pipeline, click on Author button present on the left side bar.
ii) Click on  '+' present near search bar and select pipeline > pipeline and name it.
iii) Under Activities > Move & Transform, drag and drop 'copy data' from it on the right pad.
iv) Under General GIve a name to it, leave rest as default.
v) Click on Source: Import the source dataset from blob storage for which:
	click on new and select Azure Blob Storage > select format as DelimitedText > continue
	Under set properties > provide name and create a new linked service and provide path of the dataset file.
	Leave rest options as default.
vi) Click on Sink : Provide the sink dataset, click on new > choose 'Azure SQL Database' > continue
	Now create a linked service to the dbo.autodata to store the data in the autodata table.
	Provide Server name, Database name, Username and password to connect to the database > click on create
vii) Click on Mapping > Import Schemas and match the data type of source and destination data and can change the column name if you wish.
viii) Click on settings : Only Enable staging > Staging account linked service and test connection.
ix) Under User Properties > autogenerate where you can see the source and destination location and verify it.
x) When its done click on validate, if the validation is passed then click on debug and wait for 1-2 min till the status shows as succeeded.

--> Now go to SSMS and type the query as - select * from [autodata];
	you can see the data has been copied from blob storage to the SQL Database table named 'autodata'. 	